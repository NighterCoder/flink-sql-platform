server:
  port: 8080
  servlet:
    context-path: /

####Jdbc连接池
spring:
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://127.0.0.1:3306/deer?serverTimezone=UTC&useUnicode=true&characterEncoding=utf-8&useSSL=false
    username: root
    password: 123456
    # password:

  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 50MB

  quartz:
    # 基于JDBC存储
    job-store-type: jdbc
    properties:
      org:
        quartz:
          scheduler:
            instanceName: DeerScheduler
            instanceId: AUTO
          threadPool:
            class: org.quartz.simpl.SimpleThreadPool
            threadCount: 100
            threadPriority: 5
          jobStore:
            misfireThreshold: 120000

  mail:
    username: lingzhan



hdfs:
  path: test
  upload: /tmp/deer/upload


####Spark Livy相关配置
spark:
  livy:
    url: http://localhost:8998
    # hdfs存储路径
    hive-file: /tmp/hive-site.xml
  dynamicAllocation: false



####Flink SQL执行相关配置
flink:
  env-url: conf/sql-platform-defaults.yaml
  session-name: COLLIE-FLINK
  planner: blink
  execution-type: streaming
  upload:
    path: /tmp/deer/flink/upload

####mybatis-plus配置
mybatis-plus:
  configuration:
    map-underscore-to-camel-case: true  #默认为true
    # log-impl: org.apache.ibatis.logging.stdout.StdOutImpl # 输出日志
  global-config:
    db-config:
      id-type: auto # ID自增
      logic-delete-value: 1 # 逻辑已删除值
      logic-not-delete-value: 0 # 逻辑未删除值
  type-aliases-package: com.flink.platform.web.common.entity
  mapper-locations: classpath*:mapper/*.xml

###日志
#logging:
#  level:
#    ### dao开启debug模式,输出查询sql
#    com:
#      flink:
#        platform:
#          web:
#            mapper: debug

